\documentclass[a4paper,twoside,master.tex]{subfiles}
\begin{document}
\lecture{14}{Friday, October 02, 2020}{}

In the last lecture, we discussed a division algorithm which takes a polynomial $ f \in F[X] $ in several variables and a finite set of polynomials $ G\subset F[X] $ and outputs a way of writing $ f $ as $ f = f' + r $ where $ f' $ is an $ F[X] $-linear combination of polynomials in $ G $ (where the degree of every summand is no greater than the degree of $ f $) and $ r $ is the remainder term: no term of $ r $ is divisible by any $ LT(g) $ for $ g \in G $.

\begin{ex}
    $ G = \{x^2, x^2 - y\} $. $ I = (G) = (x^2, y) $.
\end{ex}

\begin{definition}
    $ G $ is a \textit{Gr\"obner basis} for the ideal $ I = (G) $ if $ \text{LT}(I) = (\text{LT}(g) \colon g \in G) $.
\end{definition}

\begin{ex}
    $ G = \{x^2, x^2 - y\} $. It's clear that $ \text{LT}(I) = (x^2, y) $. On the other hand, the leading term of $ x^2 - y $ is $ \text{LT}(x^2 - y) = x^2 $ and $ \text{LT}(x^2) = x^2 $. Therefore, this is not a Gr\"obner basis.
\end{ex}

\begin{note}{Notation}
    The remainder $ r $ in the division algorithm is denoted $ f\mod G $.
\end{note}

\begin{claim}
    If $ I = (x^{\alpha} \colon \alpha \in S) $ is a monomial ideal (ideal generated by monomials), then this ideal consists of polynomials $ f $, each of whose terms is divisible by some $ x^{\alpha} $, $ \alpha \in S $.
\end{claim}

\begin{proof}
    $ f \in I $\textemdash we want to show that $ f $ is of this form. We will do this by induction on the number of terms in $ f $. The zero step is trivial.
    \begin{equation}
        f = \sum c_{\alpha} x^{\alpha}
    \end{equation}
    $ \text{LM}(f) $ is contained in some $ c_{\alpha} x^{\alpha} $, yet the term in $ c_{\alpha}^{\alpha} $ is divisible by $ x^{\alpha} $.
\end{proof}

\begin{claim}
    If $ I = (G) $ and $ G $ is Gr\"obner, if $ f \equiv f' (\mod I) $, then $ f\mod G = f'\mod G $.
\end{claim}

\begin{proof}
    Let $ r = f\mod G $ and $ r' = f'\mod G $. Therefore $ r - r' \equiv f-f' (\mod I) $.

    Let's write $ f = f_I + r $ and $ f' = f'_I + r' $. We can then say that $ r - r' = (f-f_I)- (f' - f'_I) \in I $.

    $ \text{LT}(r-r') \in \text{LT}(I) $. By definition of the Gr\"obner basis, $ \text{LT}(I) = (\text{LT}(G)) $. We just proved in the last proposition that $ \text{LT}(r-r') $ is therefore divisible by some $ \text{LT}(g) $ for some $ g \in G $.

    $ \text{LM}(r-r') $ is a monomial in either $ r $ or $ r' $. Say it's $ r $. This contradicts the guarantee made on the remainder by the division algorithm. The only way $ \text{LM}(r-r') = 0 $ is if $ r-r' = 0 $. 
\end{proof}
\begin{corollary}
    If $ f \in (G) $ and $ G $ is Gr\"obner, then $ f\mod G = 0 $.
\end{corollary}


Next, we want to talk about how to tell if something is a Gr\"obner basis and how we can make Gr\"obner bases out of things that aren't.

\begin{definition}
    For polynomials $ f,f' \in F[X] $, the S-polynomial (S for ``syzygy'') of $ f, f' $ is defined as
    \begin{equation}
        S(f,f') = \frac{M}{\text{LT}(f)} f - \frac{M}{\text{LT}(f')} f'
    \end{equation}
    where $ M = \text{lcm}(\text{LT}(f), \text{LT}(f')) $.
\end{definition}

\begin{theorem}[Buchberger's Criterion]
    $ G $ is Gr\"obner iff $ S(g,g') \mod G = 0 $ $ \forall g, g' \in G $.  
\end{theorem}

\begin{lemma}
    $ f_1, \cdots, f_m \in F[X] $. $ \text{LM}(f_i)= x^{\alpha} \leftrightarrow \partial(f_i) = \alpha $ and $ \partial(a_1 f_1 + \cdots + a_m f_m) < \alpha $, $ a_i \in F $, then
    \begin{equation}
        a_1 f_1 + \cdots + a_m f_m = b_1 S(f_1, s_2) + b_2 S(f_2, f_3) + \cdots + b_{m-1} S(f_{m - 1} f_{m})
    \end{equation}
    
    (note that $ \partial $ here is shorthand for ``degree'')
\end{lemma}

\begin{proof}
    $ f_i = c_i f'_i $ where $ c_i = \text{LC}(f_i) $. $ f = a_1 c_1(f'_1 - f'_2) + (a_1 c_1 + a_2 c_2)(f'_2 - f'_3) + \cdots + (a_1 c_1 + \cdots + a_m c_m) f'_m $. There are no more terms left to cancel the last term, but recall the $ \text{LM}(f) < \text{LM}(f'_m) $. We also know that $ \text{LM}(\underbrace{f'_{i+1} - f'_i}_{S(f_{i+1}, f_{i})}) < \text{LM}(f'_m) $.

    Comparing the leading coefficient of $ \text{LM}(f'_m) $, we conclude that $ a_1 c_1 + \cdots + a_m c_m = 0 $.
\end{proof}

\end{document}
